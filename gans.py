# -*- coding: utf-8 -*-
"""GANs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L4e97RFCUiPapptVe5-GYr9FG22Or75u

## 定义KID和SWD指标函数
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

import torch
from torch.utils.data import DataLoader

class KID:
    def __init__(self, real_dataset, generated_dataset, batch_size=64):
        self.real_dataset = real_dataset
        self.generated_dataset = generated_dataset
        self.batch_size = batch_size

    def compute_features(self, dataset):
        model = models.densenet121(pretrained=True)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()

        data_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)
        features = []

        with torch.no_grad():
            for inputs, _ in data_loader:
                inputs = inputs.to(device)
                outputs = model(inputs)
                features.append(outputs)

        features = torch.cat(features, dim=0)
        return features

    def compute_kid(self):
        real_features = self.compute_features(self.real_dataset)
        generated_features = self.compute_features(self.generated_dataset)

        real_mean = torch.mean(real_features, dim=0)
        generated_mean = torch.mean(generated_features, dim=0)

        real_cov = torch.matmul(real_features.t(), real_features) / real_features.shape[0]
        generated_cov = torch.matmul(generated_features.t(), generated_features) / generated_features.shape[0]

        mean_diff = real_mean - generated_mean
        cov_diff = real_cov - generated_cov

        kid = torch.norm(mean_diff) + torch.trace(cov_diff) - 2 * torch.trace(torch.sqrt(cov_diff @ cov_diff))

        return kid.item()

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

class SWD:
    def __init__(self, real_dataset, generated_dataset, batch_size=64, num_slices=10):
        self.real_dataset = real_dataset
        self.generated_dataset = generated_dataset
        self.batch_size = batch_size
        self.num_slices = num_slices

    def compute_features(self, dataset):
        model = models.densenet121(pretrained=True)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()

        data_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)
        features = []

        with torch.no_grad():
            for inputs, _ in data_loader:
                inputs = inputs.to(device)
                outputs = model(inputs)
                features.append(outputs)

        features = torch.cat(features, dim=0)
        return features

    def compute_swd(self):
        model = models.densenet121(pretrained=True)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()

        real_features = self.compute_features(self.real_dataset)
        generated_features = self.compute_features(self.generated_dataset)

        real_slices = self._slice_data(real_features)
        generated_slices = self._slice_data(generated_features)

        swd = self._compute_wasserstein_distance(real_slices, generated_slices)

        return swd

    def _slice_data(self, data):
        data_slices = []
        slice_size = data.shape[0] // self.num_slices

        for i in range(self.num_slices):
            start = i * slice_size
            end = start + slice_size
            data_slices.append(data[start:end])

        return data_slices

    def _compute_wasserstein_distance(self, real_slices, generated_slices):
        wasserstein_distance = 0.0

        for real_slice, generated_slice in zip(real_slices, generated_slices):
            real_slice = real_slice.requires_grad_()  # 使用requires_grad_()将其设置为需要梯度计算
            generated_slice = generated_slice.requires_grad_()

            distance_matrix = self._pairwise_distance(real_slice, generated_slice)

            grads_real = torch.autograd.grad(outputs=torch.sum(distance_matrix),
                                             inputs=real_slice,
                                             create_graph=True,
                                             retain_graph=True)[0]

            grads_generated = torch.autograd.grad(outputs=torch.sum(distance_matrix),
                                                  inputs=generated_slice,
                                                  create_graph=True,
                                                  retain_graph=True)[0]

            wasserstein_distance += torch.mean(grads_real - grads_generated).item()

        return wasserstein_distance

    def _pairwise_distance(self, x, y):
        x_norm = (x ** 2).sum(1).view(-1, 1)
        y_norm = (y ** 2).sum(1).view(1, -1)
        distance_matrix = torch.abs(x_norm + y_norm - 2.0 * torch.matmul(x, y.t()))

        return distance_matrix

import torchvision
from torchvision import transforms

# 加载CIFAR-10数据集
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 标准化图像
])

cifar_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# 定义KID指标类
kid_metric = KID(cifar_dataset, cifar_dataset)
kid_score = kid_metric.compute_kid()
print("KID score:", kid_score)

# 定义SWD指标类
swd_metric = SWD(cifar_dataset, cifar_dataset)
swd_score = swd_metric.compute_swd()
print("SWD score:", swd_score)

"""## Vanilla GAN × CIFAR-10"""

import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets
from torchvision.transforms import ToTensor, Normalize, Compose
from torchvision.utils import save_image, make_grid
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 定义模型超参数
latent_dim = 64
hidden_dim = 256
image_dim = 32*32*3  # CIFAR-10 images are 32x32 and colored
num_epochs = 100
batch_size = 128
learning_rate = 0.0002

# 加载数据集
transform = Compose([
    ToTensor(),
    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_data = datasets.CIFAR10(
    root='.',
    train=True,
    download=True,
    transform=transform
)

test_data = datasets.CIFAR10(
    root='.',
    train=False,
    download=True,
    transform=transform
)

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

# 定义生成器和判别器
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim * 2, hidden_dim * 4),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim * 4, image_dim),
            nn.Tanh(),
        )

    def forward(self, x):
        return self.main(x).view(-1, 3, 32, 32)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(image_dim, hidden_dim * 4),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim * 4, hidden_dim * 2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid(),
        )

    def forward(self, x):
        return self.main(x.view(-1, image_dim))

# 初始化模型和优化器
G = Generator().to(device)
D = Discriminator().to(device)

criterion = nn.BCELoss()
d_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)
g_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)

def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)

def reset_grad():
    d_optimizer.zero_grad()
    g_optimizer.zero_grad()

kid_scores = []
swd_scores = []

# 训练模型
total_step = len(train_loader)
for epoch in range(num_epochs):
    for i, (images, _) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")):
        # 确保最后一批次的尺寸正确
        if images.shape[0] != batch_size:
            continue

        real_images = images.to(device)
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        # 训练判别器
        outputs = D(real_images)
        d_loss_real = criterion(outputs, real_labels)
        real_score = outputs

        z = torch.randn(batch_size, latent_dim).to(device)
        fake_images = G(z)
        outputs = D(fake_images.detach())
        d_loss_fake = criterion(outputs, fake_labels)
        fake_score = outputs

        d_loss = d_loss_real + d_loss_fake
        reset_grad()
        d_loss.backward()
        d_optimizer.step()

        # 训练生成器
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_images = G(z)
        outputs = D(fake_images)
        g_loss = criterion(outputs, real_labels)

        reset_grad()
        g_loss.backward()
        g_optimizer.step()

    print(f"d_loss: {d_loss.item()}, g_loss: {g_loss.item()}")

    if (epoch + 1) % 10 == 0:
      # 每10个epoch结束后，随机展示5个生成的图像
      z = torch.randn(5, latent_dim).to(device)
      fake_images = G(z)
      fake_images = fake_images.view(-1, 3, 32, 32)
      fake_images = denorm(fake_images)

      fig, ax = plt.subplots(1, 5, figsize=(15, 3))
      for i, img in enumerate(fake_images.detach().cpu().numpy()):
          ax[i].imshow(np.transpose(img, (1, 2, 0)))
          ax[i].axis('off')
      plt.show()

      z = torch.randn(len(test_data), latent_dim).to(device)
      fake_images = G(z)
      fake_images = fake_images.view(-1, 3, 32, 32)
      fake_images = denorm(fake_images)

      # 创建生成的数据集对象
      generated_data = TensorDataset(fake_images, torch.zeros(len(fake_images)))

      # 创建KID和SWD对象并计算指标
      kid_calculator = KID(real_dataset=test_data, generated_dataset=generated_data)
      swd_calculator = SWD(real_dataset=test_data, generated_dataset=generated_data)
      kid_score = kid_calculator.compute_kid()
      swd_score = swd_calculator.compute_swd()

      # 在每个epoch结束后打印KID和SWD指标
      print(f"Epoch [{epoch+1}/{num_epochs}], KID: {kid_score}, SWD: {swd_score}")

"""## WGAN × CIFAR-10"""

import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets
from torchvision.transforms import ToTensor, Normalize, Compose
from torchvision.utils import save_image, make_grid
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('device = ', device)

# 定义模型超参数
latent_dim = 100
num_epochs = 100
batch_size = 64
learning_rate = 0.0002
clip_value = 0.01  # WGAN cliping value

# 加载数据集
transform = Compose([
    ToTensor(),
    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_data = datasets.CIFAR10(
    root='.',
    train=True,
    download=True,
    transform=transform
)

test_data = datasets.CIFAR10(
    root='.',
    train=False,
    download=True,
    transform=transform
)

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        # 输入为100维的随机噪声向量
        self.fc = nn.Linear(100, 512*2*2) # 输出为512x2x2
        self.deconv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1) # 输出为256x4x4
        self.deconv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1) # 输出为128x8x8
        self.deconv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1) # 输出为64x16x16
        self.deconv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1) # 输出为3x32x32
        self.bn1 = nn.BatchNorm2d(256)
        self.bn2 = nn.BatchNorm2d(128)
        self.bn3 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()

    def forward(self, z):
        z = self.fc(z)
        z = z.view(-1, 512, 2 ,2)
        z = self.relu(self.bn1(self.deconv1(z)))
        z = self.relu(self.bn2(self.deconv2(z)))
        z = self.relu(self.bn3(self.deconv3(z)))
        z = self.tanh(self.deconv4(z))
        return z

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        # 输入为3x32x32的CIFAR-10图像
        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1) # 输出为64x16x16
        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1) # 输出为128x8x8
        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1) # 输出为256x4x4
        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1) # 输出为512x2x2
        self.fc = nn.Linear(512*2*2, 1) # 输出为1
        self.lrelu = nn.LeakyReLU(0.2)

    def forward(self, x):
        x = self.lrelu(self.conv1(x))
        x = self.lrelu(self.conv2(x))
        x = self.lrelu(self.conv3(x))
        x = self.lrelu(self.conv4(x))
        x = x.view(-1, 512*2*2)
        x = self.fc(x)
        return x

# 初始化模型和优化器
G = Generator().to(device)
D = Discriminator().to(device)

d_optimizer = torch.optim.RMSprop(D.parameters(), lr=learning_rate)
g_optimizer = torch.optim.RMSprop(G.parameters(), lr=learning_rate)

def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)

def reset_grad():
    d_optimizer.zero_grad()
    g_optimizer.zero_grad()

# 训练模型
total_step = len(train_loader)
for epoch in range(num_epochs):
    for i, (images, _) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")):
        # 确保最后一批次的尺寸正确
        if images.shape[0] != batch_size:
            continue

        real_images = images.to(device)

        # 训练判别器
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_images = G(z)
        
        real_outputs = D(real_images)
        fake_outputs = D(fake_images.detach())

        d_loss = -torch.mean(real_outputs) + torch.mean(fake_outputs)
        reset_grad()
        d_loss.backward()
        d_optimizer.step()

        # Clip weights of discriminator
        for p in D.parameters():
            p.data.clamp_(-clip_value, clip_value)

        # 训练生成器
        if i % 5 == 0:
            z = torch.randn(batch_size, latent_dim).to(device)
            fake_images = G(z)
            outputs = D(fake_images)
            g_loss = -torch.mean(outputs)

            reset_grad()
            g_loss.backward()
            g_optimizer.step()

    print(f"d_loss: {d_loss.item()}, g_loss: {g_loss.item()}")

    if (epoch + 1) % 10 == 0:
        # 每5个epoch结束后，随机展示5个生成的图像
        z = torch.randn(5, latent_dim).to(device)
        fake_images = G(z)
        fake_images = fake_images.view(-1, 3, 32, 32)
        fake_images = denorm(fake_images)

        fig, ax = plt.subplots(1, 5, figsize=(15, 3))
        for i, img in enumerate(fake_images.detach().cpu().numpy()):
            ax[i].imshow(np.transpose(img, (1, 2, 0)))
            ax[i].axis('off')
        plt.show()
        plt.savefig(f'WGAN_[{epoch}].png')

        z = torch.randn(len(test_data), latent_dim).to(device)
        fake_images = G(z)
        fake_images = fake_images.view(-1, 3, 32, 32)
        fake_images = denorm(fake_images)

        # 创建生成的数据集对象
        generated_data = TensorDataset(fake_images.detach().cpu(), torch.zeros(len(fake_images)))

        # 创建KID和SWD对象并计算指标
        kid_calculator = KID(real_dataset=test_data, generated_dataset=generated_data)
        swd_calculator = SWD(real_dataset=test_data, generated_dataset=generated_data)
        kid_score = kid_calculator.compute_kid()
        swd_score = swd_calculator.compute_swd()

        # 在每个epoch结束后打印KID和SWD指标
        print(f"Epoch [{epoch+1}/{num_epochs}], KID: {kid_score}, SWD: {swd_score}")

"""## WGAN-GP × CIFAR-10"""

import torch
from torch import nn, autograd
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets
from torchvision.transforms import ToTensor, Normalize, Compose
from torchvision.utils import save_image, make_grid
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('device = ', device)

# 定义模型超参数
latent_dim = 100
num_epochs = 100
batch_size = 64
learning_rate = 0.0002
lambda_gp = 10  # Gradient penalty lambda hyperparameter

# 加载数据集
transform = Compose([
    ToTensor(),
    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_data = datasets.CIFAR10(
    root='.',
    train=True,
    download=True,
    transform=transform
)

test_data = datasets.CIFAR10(
    root='.',
    train=False,
    download=True,
    transform=transform
)

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        # 输入为100维的随机噪声向量
        self.fc = nn.Linear(100, 512*2*2) # 输出为512x2x2
        self.deconv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1) # 输出为256x4x4
        self.deconv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1) # 输出为128x8x8
        self.deconv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1) # 输出为64x16x16
        self.deconv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1) # 输出为3x32x32
        self.bn1 = nn.BatchNorm2d(256)
        self.bn2 = nn.BatchNorm2d(128)
        self.bn3 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()

    def forward(self, z):
        z = self.fc(z)
        z = z.view(-1, 512, 2 ,2)
        z = self.relu(self.bn1(self.deconv1(z)))
        z = self.relu(self.bn2(self.deconv2(z)))
        z = self.relu(self.bn3(self.deconv3(z)))
        z = self.tanh(self.deconv4(z))
        return z

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        # 输入为3x32x32的CIFAR-10图像
        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1) # 输出为64x16x16
        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1) # 输出为128x8x8
        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1) # 输出为256x4x4
        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1) # 输出为512x2x2
        self.fc = nn.Linear(512*2*2, 1) # 输出为1
        self.lrelu = nn.LeakyReLU(0.2)

    def forward(self, x):
        x = self.lrelu(self.conv1(x))
        x = self.lrelu(self.conv2(x))
        x = self.lrelu(self.conv3(x))
        x = self.lrelu(self.conv4(x))
        x = x.view(-1, 512*2*2)
        x = self.fc(x)
        return x

G = Generator().to(device)
D = Discriminator().to(device)

d_optimizer = torch.optim.RMSprop(D.parameters(), lr=learning_rate)
g_optimizer = torch.optim.RMSprop(G.parameters(), lr=learning_rate)

def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)

def reset_grad():
    d_optimizer.zero_grad()
    g_optimizer.zero_grad()

# 计算梯度惩罚
def compute_gradient_penalty(D, real_samples, fake_samples):
    alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1))).to(device)
    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)
    d_interpolates = D(interpolates)
    fake = torch.ones(real_samples.shape[0], 1).to(device)
    gradients = autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=fake,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
    )[0]
    gradients = gradients.view(gradients.size(0), -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gradient_penalty

# 训练模型
total_step = len(train_loader)
for epoch in range(num_epochs):
    for i, (images, _) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")):
        # 确保最后一批次的尺寸正确
        if images.shape[0] != batch_size:
            continue

        real_images = images.to(device)

        # 训练判别器
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_images = G(z)
        
        real_outputs = D(real_images)
        fake_outputs = D(fake_images.detach())

        # 计算梯度惩罚
        gradient_penalty = compute_gradient_penalty(D, real_images.data, fake_images.data)

        d_loss = -torch.mean(real_outputs) + torch.mean(fake_outputs) + lambda_gp * gradient_penalty

        reset_grad()
        d_loss.backward()
        d_optimizer.step()

        # 训练生成器
        if i % 5 == 0:
            z = torch.randn(batch_size, latent_dim).to(device)
            fake_images = G(z)
            outputs = D(fake_images)
            g_loss = -torch.mean(outputs)

            reset_grad()
            g_loss.backward()
            g_optimizer.step()

    print(f"d_loss: {d_loss.item()}, g_loss: {g_loss.item()}")

    if (epoch + 1) % 10 == 0:
        # 每5个epoch结束后，随机展示5个生成的图像
        z = torch.randn(5, latent_dim).to(device)
        fake_images = G(z)
        fake_images = fake_images.view(-1, 3, 32, 32)
        fake_images = denorm(fake_images)

        fig, ax = plt.subplots(1, 5, figsize=(15, 3))
        for i, img in enumerate(fake_images.detach().cpu().numpy()):
            ax[i].imshow(np.transpose(img, (1, 2, 0)))
            ax[i].axis('off')
        plt.show()
        plt.savefig(f'WGAN-GP_[{epoch}].png')

        z = torch.randn(len(test_data), latent_dim).to(device)
        fake_images = G(z)
        fake_images = fake_images.view(-1, 3, 32, 32)
        fake_images = denorm(fake_images)

        # 创建生成的数据集对象
        generated_data = TensorDataset(fake_images.detach().cpu(), torch.zeros(len(fake_images)))

        # 创建KID和SWD对象并计算指标
        kid_calculator = KID(real_dataset=test_data, generated_dataset=generated_data)
        swd_calculator = SWD(real_dataset=test_data, generated_dataset=generated_data)
        kid_score = kid_calculator.compute_kid()
        swd_score = swd_calculator.compute_swd()

        # 在每个epoch结束后打印KID和SWD指标
        print(f"Epoch [{epoch+1}/{num_epochs}], KID: {kid_score}, SWD: {swd_score}")